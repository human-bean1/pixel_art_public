{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "460a90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import requests\n",
    "import os\n",
    "from bing_image_downloader import downloader\n",
    "import re\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageStat\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c92a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = \"\"\n",
    "CONSUMER_SECRET = \"\"\n",
    "\n",
    "ACCESS_TOKEN = \"\"\n",
    "ACCESS_SECRET = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "55386737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up the Twitter API\n",
    "def getAPI():\n",
    "    \n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    return api\n",
    "\n",
    "\n",
    "# Populates an empty dictionary with tweets\n",
    "def get_tweets(verbose):\n",
    "    \n",
    "    tweet_dict = {}  # initialize dictionary for tweets\n",
    "    count = 0\n",
    "    api = getAPI()\n",
    "    # API.search_tweets(q, *, geocode, lang, locale, result_type, count, until, since_id, max_id, include_entities)ÔÉÅ\n",
    "    dream_tweets = api.search_tweets('dream', count=10) # change back to 100 later, for testing just use small number\n",
    "    for tweet in dream_tweets:\n",
    "        if(len(tweet.user.location) != 0):\n",
    "            tweet_dict[count] = {'location': tweet.user.location, 'id': tweet.id, 'profile_url': tweet.user.profile_image_url, \\\n",
    "                                 'text': tweet.text, 'geo': tweet.geo, 'source': tweet.source, 'created_at': tweet.created_at, \\\n",
    "                                'lang': tweet.lang }\n",
    "            \n",
    "            count += 1\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(\"number of tweets: \"+ str(count))\n",
    "    \n",
    "    return tweet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4c836d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads satellite images from Bing of location from Tweet\n",
    "\n",
    "def get_satellite_image(tweet):\n",
    "\n",
    "    location = tweet['location']\n",
    "\n",
    "    # note: if 'location' contains special characters, might throw error for creation of directory in dataset\n",
    "    escaped = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", location) # note: cleans out non-English\n",
    "    location = escaped\n",
    "    tweet['location'] = location # location string updated for referencing dir later\n",
    "    \n",
    "    query_string = location + ' satellite map'\n",
    "    \n",
    "    # check and clear old dataset if exists\n",
    "    dirpath = Path('dataset') / str(query_string)\n",
    "    if dirpath.exists() and dirpath.is_dir():\n",
    "        shutil.rmtree(dirpath)\n",
    "    \n",
    "    downloader.download(query_string, limit=3, output_dir='dataset', adult_filter_off=True, force_replace=False, timeout=60, verbose=False)\n",
    " \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "87f68c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads twitter user's profile picture given tweet and count number \n",
    "def get_twitter_profile(tweet, count):    \n",
    "    \n",
    "    image_url = tweet['profile_url']\n",
    "    img_data = requests.get(image_url).content\n",
    "    \n",
    "    # downloads to 'profile' directory\n",
    "    while True:\n",
    "        try:\n",
    "            with open('./profile/' + str(count) + '.png', 'wb') as handler:\n",
    "                handler.write(img_data)\n",
    "            break\n",
    "        except:\n",
    "            print(\"Oops! Twitter profile img failed to download. \")\n",
    "    \n",
    "    return 1\n",
    "\n",
    "# combines the satellite image and user profile associated with a tweet\n",
    "def combine_images(tweet, count, isCrop): # boolean for 'isCrop'\n",
    "        \n",
    "    image = Image.open('./profile/' + str(count) + '.png') \n",
    "    image = image.resize((1000,1000))\n",
    "    \n",
    "    # image 2: open satellite map from folder\n",
    "    query_string = tweet['location'] + ' satellite map'\n",
    "    path = os.path.join(\"dataset\", query_string) # folder path\n",
    "    dir_list = os.listdir(path)\n",
    "    path = os.path.join(\"dataset\", query_string, dir_list[0]) # image 1 in folder path\n",
    "    satelliteIm = Image.open(path)\n",
    "    satelliteIm = satelliteIm.resize((1000,1000))\n",
    "    \n",
    "    if isCrop == True:\n",
    "        image = crop_most_edges(image, False)\n",
    "        satelliteIm = crop_most_edges(satelliteIm, False)    \n",
    " \n",
    "   # convert images to ' ' mode\n",
    "    image = image.convert('RGBA')\n",
    "    satelliteIm = satelliteIm.convert('RGBA')\n",
    " \n",
    "    # alpha is 0.5, \n",
    "    im3 = Image.blend(image, satelliteIm, 0.5)\n",
    " \n",
    "    # return combined image\n",
    "    return im3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "59cf9038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge detection, crop based on which section has most edges\n",
    "def crop_most_edges(image, verbose):\n",
    "\n",
    "    # Converting the image to grayscale, as edge detection\n",
    "    # requires input image to be of mode = Grayscale (L)\n",
    "    image = image.convert(\"L\")\n",
    "    imageWithEdges = image.filter(ImageFilter.FIND_EDGES)\n",
    "\n",
    "    # crop options\n",
    "    left, top, right, bottom = 0, 0, 1000, 400 # top half\n",
    "    crop_1 = imageWithEdges.crop((left, top, right, bottom))\n",
    "\n",
    "    left, top, right, bottom = 0, 400, 1000, 800 # middle half\n",
    "    crop_2 = imageWithEdges.crop((left, top, right, bottom))\n",
    "    \n",
    "    left, top, right, bottom = 0, 600, 1000, 1000 # bottom half\n",
    "    crop_3 = imageWithEdges.crop((left, top, right, bottom))\n",
    "\n",
    "    # calculate stats\n",
    "    stat_1 = ImageStat.Stat(crop_1)\n",
    "    stat_2 = ImageStat.Stat(crop_2)\n",
    "    stat_3 = ImageStat.Stat(crop_3)\n",
    "    stat = ImageStat.Stat(imageWithEdges)\n",
    "    if verbose == True:\n",
    "        print(\"crop_1: \" + str(stat_1.mean))\n",
    "        print(\"crop_2: \" + str(stat_2.mean))\n",
    "        print(\"crop_3: \" + str(stat_3.mean))\n",
    "    \n",
    "    # decide which crop option to use\n",
    "    option = max(stat_1.mean[0], stat_2.mean[0], stat_3.mean[0])\n",
    "    if option == stat_2:\n",
    "        image = image.crop((0, 400, 1000, 800)) # middle pt\n",
    "    elif option == stat_1:\n",
    "        image = image.crop((0, 0, 1000, 400)) # top pt\n",
    "    else:\n",
    "        image = image.crop((0, 600, 1000, 1000)) # bottom pt\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0188f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Call draw Method to add 2D graphics in an image\n",
    "\n",
    "def add_text(image, tweet):\n",
    "    \n",
    "    imageNew = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Custom font style and font size\n",
    "    myFont = ImageFont.truetype('simsun.ttc', 30)\n",
    "    \n",
    "    # Add Text to an image\n",
    "    imageNew.text((10, 10), tweet['location'], font=myFont, fill =(0, 0, 255)) # text is location name\n",
    "    imageNew.text((10, 50), tweet['text'], font=myFont, fill =(0, 0, 255))\n",
    "    imageNew.text((10, 90), str(tweet['created_at']), font=myFont, fill =(0, 0, 255))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21167d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Runs program on a single tweet\n",
    "# inputs, # of tickets to print, # of tweets to collect\n",
    "\n",
    "# populates dictionary with tweets\n",
    "tweet_dict = get_tweets(True)\n",
    "\n",
    "# downloads satellite image given one tweet\n",
    "get_satellite_image(tweet_dict[0])\n",
    "\n",
    "# downloads twitter user profile picture\n",
    "get_twitter_profile(tweet_dict[0], 0)\n",
    "\n",
    "# combines the two images\n",
    "image = combine_images(tweet_dict[0], 0, True)\n",
    "\n",
    "# give image path and tweet to add text \n",
    "image = add_text(image, tweet_dict[0])\n",
    "image.show()\n",
    "\n",
    "# converting to jpg\n",
    "rgb_im = image.convert(\"RGB\")\n",
    "rgb_im.save(\"ticket.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "917e0f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from skimage import io, color\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "\n",
    "from numpy import array\n",
    "from sklearn.decomposition import PCA\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4243368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99a3ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to initialize the super pixels, of the form - [h,w,l,a,b].\n",
    "class SuperPixels(object):\n",
    "\n",
    "    def __init__(self, h, w, r, c, l=0, a=0, b=0):\n",
    "        self.update(h, w, r, c, l, a, b)\n",
    "        self.pixels = [] # tracks associated input pixels\n",
    "\n",
    "    def update(self, h, w, r, c, l, a, b):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.r = r # row in initial grid\n",
    "        self.c = c # column in initial grid\n",
    "        self.l = l\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "178704af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an object of class SuperPixel\n",
    "def make_superPixel(h, w, img, r, c):\n",
    "    return SuperPixels(h, w, r, c, img[h,w][0],img[h,w][1],img[h,w][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "134756f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient;  G(x,y) = ‚ÄñI(x+ 1,y)‚àíI(x‚àí1,y)‚Äñ¬≤+‚ÄñI(x,y+ 1)‚àíI(x,y‚àí1)‚Äñ¬≤\n",
    "def calc_gradient(h, w,img,img_w,img_h):\n",
    "    \n",
    "    l = pow(img[h, w+1][0] - img[h, w - 1][0] , 2) #\n",
    "    a = pow(img[h, w+1][1] - img[h, w - 1][1] , 2)\n",
    "    b = pow(img[h, w+1][2] - img[h, w - 1][2] , 2)\n",
    "    x_diff = math.sqrt(l + a + b)\n",
    "    \n",
    "    l = pow(img[h + 1, w][0] - img[h - 1, w][0] , 2)\n",
    "    a = pow(img[h + 1, w][1] - img[h - 1, w][1] , 2)\n",
    "    b = pow(img[h + 1, w][2] - img[h - 1, w][2] , 2)\n",
    "    y_diff = math.sqrt(l + a + b)\n",
    "    \n",
    "    grad = x_diff + y_diff\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33d8d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign clusters based on gradient\n",
    "def perturb_center(clusters, img):\n",
    "    for c in clusters:\n",
    "        center_gradient = calc_gradient(c.h, c.w,img,img_w,img_h)\n",
    "        w = c.w\n",
    "        h = c.h\n",
    "        for x in range(-1, 2):\n",
    "            for y in range(-1, 2):\n",
    "                W = w + x\n",
    "                H = h + y\n",
    "            \n",
    "                if H == img_h or W == img_w: # might need to increase to H-1,W-1\n",
    "                    new_gradient = math.inf\n",
    "                else:\n",
    "                    new_gradient = calc_gradient(H,W, img,img_w,img_h)\n",
    "                    \n",
    "                if new_gradient < center_gradient: # reassign to lower gradient position\n",
    "                    c.update(H, W, c.r, c.c, img[H,W][0], img[H,W][1],img[H,W][2])\n",
    "                    center_gradient = new_gradient # keeps track of lowest found in neighborhood so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d900bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines initial cluster centers distanced at S\n",
    "def initial_cluster_center(S,img,img_h,img_w): \n",
    "    clusters = []\n",
    "    h = S // 2 \n",
    "    w = S // 2\n",
    "    r = 0\n",
    "    c = 0\n",
    "    while h < img_h:\n",
    "        while w < img_w:\n",
    "            clusters.append(make_superPixel(h, w, img, r, c))\n",
    "            c += 1\n",
    "            w += S\n",
    "        w = S // 2\n",
    "        h += S\n",
    "        r += 1\n",
    "        c = 0  \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79d2179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaces superpixel representative color w/ mean of the input pixels from a given bilateral filtered version\n",
    "def update_mean_color(clusters, bilateral):\n",
    "    new_means = []\n",
    "    for c in clusters:\n",
    "        sum_l, sum_a, sum_b = 0,0,0\n",
    "        n = len(c.pixels)\n",
    "        \n",
    "        for p in c.pixels:           \n",
    "            sum_l += bilateral[p[0],p[1]][0]\n",
    "            sum_a += bilateral[p[0],p[1]][1]\n",
    "            sum_b += bilateral[p[0],p[1]][2]\n",
    "            \n",
    "        avg_l = sum_l / n # don't use floor division for LAB values\n",
    "        avg_a = sum_a / n\n",
    "        avg_b = sum_b / n\n",
    "        new_means.append([avg_l, avg_a, avg_b])\n",
    "\n",
    "    return new_means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd3892f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaces cluster center's (x,y) position w/ mean of the input pixels and avg. color\n",
    "def update_cluster_mean(clusters, image):\n",
    "    for c in clusters:\n",
    "        sum_h = sum_w = number = 0\n",
    "        sum_l, sum_a, sum_b = 0,0,0\n",
    "        n = len(c.pixels)\n",
    "        \n",
    "        for p in c.pixels:\n",
    "            sum_h += p[0]\n",
    "            sum_w += p[1]            \n",
    "            sum_l += image[p[0],p[1]][0]\n",
    "            sum_a += image[p[0],p[1]][1]\n",
    "            sum_b += image[p[0],p[1]][2]\n",
    "            \n",
    "        H = sum_h // n\n",
    "        W = sum_w // n\n",
    "        avg_l = sum_l / n # don't use floor division for LAB values\n",
    "        avg_a = sum_a / n\n",
    "        avg_b = sum_b / n\n",
    "        c.update(H, W, c.r, c.c, avg_l, avg_a, avg_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42d21f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts LAB images back to RGB and save it\n",
    "def lab2rgb(path, lab_arr):\n",
    "    rgb_arr = color.lab2rgb(lab_arr)\n",
    "    io.imsave(path, rgb_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e39ddcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the color of each pixel in a cluster by the color of the cluster's center\n",
    "# delete \n",
    "def avg_color_cluster(img,name,clusters):\n",
    "    image = np.copy(img)\n",
    "        \n",
    "    for c in clusters:\n",
    "        for p in c.pixels:\n",
    "            image[p[0],p[1]][0] = c.l\n",
    "            image[p[0],p[1]][1] = c.a\n",
    "            image[p[0],p[1]][2] = c.b\n",
    "        # To change the color of cluster center to Black\n",
    "        image[c.h, c.w][0] = 0\n",
    "        image[c.h, c.w][1] = 0\n",
    "        image[c.h, c.w][2] = 0        \n",
    "        \n",
    "    lab2rgb(name, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ad89bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign pixels to center w/ best color & position proximity in a 2S x 2S neighborhood\n",
    "def assign_pixels(clusters, S, img, img_h, img_w):\n",
    "    \n",
    "    prev = {} # tracks clusters found before best cluster identified\n",
    "    \n",
    "    # initialize the distance between pixels and cluster center as infinity\n",
    "    dis = np.full((img_h, img_w), np.inf) \n",
    "    \n",
    "    for c in clusters:\n",
    "        for h in range(c.h - 2 * S, c.h + 2 * S):       \n",
    "            if h < 0 or h >= img_h: continue           # check image height boundaries\n",
    "            for w in range(c.w - 2 * S, c.w + 2 * S):  # 2S x 2S neighborhood\n",
    "                if w < 0 or w >= img_w: continue       # check image width boundaries\n",
    "                l, a, b = img[h,w]                     # load input pixel\n",
    "                Dc = math.sqrt(math.pow(l - c.l, 2) + math.pow(a - c.a, 2) + math.pow(b - c.b, 2))\n",
    "                Dp = math.sqrt(math.pow(h - c.h, 2) + math.pow(w - c.w, 2))\n",
    "                D = Dc + (Dp * m / S) # don't use floor division\n",
    "                # check if closest center found so far\n",
    "                if D < dis[h,w]:\n",
    "                    if (h, w) in prev:\n",
    "                        prev[(h, w)].pixels.remove((h, w)) # removes an input pixel prev. associated w/ another cluster\n",
    "                        prev[(h, w)] = c # re-assigned cluster\n",
    "                        c.pixels.append((h, w))\n",
    "                    else:\n",
    "                        prev[(h, w)] = c\n",
    "                        c.pixels.append((h, w))\n",
    "                    dis[h, w] = D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d53a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize color palette given specified size\n",
    "def makePalette(size, initial_color):\n",
    "    arr = np.zeros((size, 3))\n",
    "    arr[0] = initial_color\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eeccbccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used during palette initialization\n",
    "# assign superpixel to color in palette\n",
    "def assign_color(clusters, palette, color):\n",
    "    for c in clusters:\n",
    "        c.l = palette[color][0]\n",
    "        c.a = palette[color][1]\n",
    "        c.b = palette[color][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f047fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate each superpixel w/ a palette color\n",
    "def assign_pal_color(clusters, palette, pal_size):\n",
    "    this_probs = []\n",
    "    for c in clusters:\n",
    "        c_prob = 0\n",
    "        y = [] # store probabilities for each color \n",
    "        for count, p in enumerate(palette):\n",
    "                        \n",
    "            # calculate diff in LAB space\n",
    "            norm = math.sqrt(math.pow(c.l - p[0], 2) + math.pow(c.a - p[1], 2) + math.pow(c.b - p[2], 2))\n",
    "            x = (norm / T) * -1\n",
    "            x = 2*x # test\n",
    "            new_prob = math.pow(math.e, x) \n",
    "            y.append(new_prob)\n",
    "                       \n",
    "            # associate if largest probability found\n",
    "            if new_prob > c_prob:\n",
    "                c.l = p[0]\n",
    "                c.a = p[1]\n",
    "                c.b = p[2] # assigns superpixel w/ palette color \n",
    "                c_prob = new_prob # track latest greatest probability found so far    \n",
    "            \n",
    "        y = normalize_probs(y)\n",
    "        this_probs.append(y)\n",
    "            \n",
    "#     print(\"pal this_probs:\", this_probs)\n",
    "    \n",
    "    return this_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2fa05383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize probabilities\n",
    "# takes in an array of probabilities and makes sure they sum to 1\n",
    "def normalize_probs(probs):\n",
    "    norm_probs = []\n",
    "    all_probs = sum(probs)\n",
    "    for p in probs:\n",
    "        x = p / all_probs\n",
    "        norm_probs.append(x)\n",
    "    return norm_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d17b1b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#refine color palette\n",
    "def refine_pal(clusters, palette, size, new_means, probs): \n",
    "    \n",
    "    # best way to refine palette?\n",
    "    new_pal = []\n",
    "    \n",
    "    for i in range(size):\n",
    "        # calculate weighted avg of superpixel colors to a palette color\n",
    "        # sum up each product of superpixel's color and weight (associated probability)\n",
    "        sum_l = 0\n",
    "        sum_a = 0\n",
    "        sum_b = 0\n",
    "        sum_weights = 0.00001\n",
    "        \n",
    "        lst = get_clusters(clusters, probs)\n",
    "        for count, c in enumerate(clusters): # get bilateral mean color\n",
    "            if lst[count] == i:\n",
    "                prob = probs[count][i] # associated probability\n",
    "\n",
    "                l = new_means[count][0] * prob\n",
    "                a = new_means[count][1] * prob\n",
    "                b = new_means[count][2] * prob\n",
    "            \n",
    "                sum_l += l\n",
    "                sum_a += a\n",
    "                sum_b += b\n",
    "                sum_weights += prob\n",
    "        # weighted avg\n",
    "        new_l = sum_l / sum_weights\n",
    "        new_a = sum_a / sum_weights\n",
    "        new_b = sum_b / sum_weights \n",
    "           \n",
    "        new_pal.append([new_l, new_a, new_b])\n",
    "    return new_pal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "503fd7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get clusters associated with palette color\n",
    "def get_clusters(clusters, probs):\n",
    "    pal_indices = []\n",
    "    for x in probs:\n",
    "        an_array = np.array(x)\n",
    "        index = np.argmax(an_array) # index of palette color\n",
    "        pal_indices.append(index)\n",
    "    return pal_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4ded526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if palette converged\n",
    "def if_converg(prev_pal, palette):\n",
    "#     tolerance = 0.01\n",
    "    tolerance = 0.1\n",
    "    diffs = []\n",
    "    for count, p in enumerate(palette):       \n",
    "        l = math.pow(prev_pal[count][0] - p[0], 2)\n",
    "        a = math.pow(prev_pal[count][1] - p[1], 2)\n",
    "        b = math.pow(prev_pal[count][2] - p[2], 2)\n",
    "        diff = math.sqrt(l+a+b)\n",
    "        diffs.append(diff)\n",
    "    print(\"pal diff\")\n",
    "    print(max(diffs))\n",
    "    if max(diffs) < tolerance: \n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9abfa752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# laplacian smoothing: adjust superpixel center closer to avg of its 4-connected neighbors\n",
    "\n",
    "def laplacian(clusters):\n",
    "    x = len(clusters)\n",
    "    row = clusters[x-1].r + 1 # number of rows\n",
    "    col = clusters[x-1].c + 1 # number of columns\n",
    "      \n",
    "    for count, c in enumerate(clusters):\n",
    "\n",
    "        # skip first and last row; # skip left and right column\n",
    "        if c.r == 0 or c.r == (row-1):\n",
    "            continue \n",
    "        if c.c == 0 or c.c == (col-1):\n",
    "            continue    \n",
    "\n",
    "        # get h & w from 4-connected neighbors\n",
    "        lh, lw = clusters[count - 1].h, clusters[count - 1].w\n",
    "        rh, rw = clusters[count + 1].h, clusters[count + 1].w\n",
    "        th, tw = clusters[count - col].h, clusters[count - col].w\n",
    "                \n",
    "        bh, bw = clusters[count + col].h, clusters[count + col].w\n",
    "\n",
    "        # calculate displacement\n",
    "        x_avg = (lw + rw + tw + bw) / 4\n",
    "        y_avg = (lh + rh + th + bh) / 4 \n",
    "        x_diff = c.w - x_avg\n",
    "        y_diff = c.h - y_avg\n",
    "\n",
    "        # move center closer to avg \n",
    "        c.w = math.floor(c.w + (x_diff * 0.4))\n",
    "        c.h = math.floor(c.h + (y_diff * 0.4))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b0a4df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand palette\n",
    "def expand_pal(palette, size, new_means, probs):\n",
    "    \n",
    "    new_pal = []\n",
    "    \n",
    "    means_lst = run_means_assign(palette, new_means, probs) #\n",
    "    \n",
    "    # iterate over each color in palette\n",
    "    for count, p in enumerate(palette):\n",
    "        # get centroids\n",
    "        if not means_lst[count]: continue \n",
    "        if len(means_lst[count])==1: continue\n",
    "        centers_lst = six_centroids(means_lst[count]) ##\n",
    "    new_pal = centers_lst\n",
    "\n",
    "    return new_pal  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e9c340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates two centroids for a list of LAB colors\n",
    "def six_centroids(data): \n",
    "    linkage_data = linkage(data, method='ward', metric='euclidean')\n",
    "    dendrogram(linkage_data)\n",
    "    hierarchical_cluster = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='ward')\n",
    "    labels = hierarchical_cluster.fit_predict(data) \n",
    "    \n",
    "    first = []\n",
    "    second = []\n",
    "    third = []\n",
    "    four = []\n",
    "    five = []\n",
    "    six = []\n",
    "\n",
    "    for count, l in enumerate(labels):\n",
    "        if l == 0:\n",
    "            first.append(data[count])\n",
    "        if l == 1:\n",
    "            second.append(data[count])\n",
    "        if l == 2:\n",
    "            third.append(data[count])\n",
    "        if l == 3:\n",
    "            four.append(data[count])\n",
    "        if l == 4:\n",
    "            five.append(data[count])\n",
    "        if l == 5:\n",
    "            six.append(data[count])\n",
    " \n",
    "    c_1 = np.array(first).mean(axis=0)\n",
    "    c_2 = np.array(second).mean(axis=0)\n",
    "    c_3 = np.array(third).mean(axis=0)\n",
    "    c_4 = np.array(four).mean(axis=0)\n",
    "    c_5 = np.array(five).mean(axis=0)\n",
    "    c_6 = np.array(six).mean(axis=0)\n",
    "    \n",
    "    centroids = []\n",
    "    centroids.extend([c_1, c_2, c_3, c_4, c_5, c_6])\n",
    "    # returns the centroids of the list \n",
    "    return centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad66583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run means_assign on each palette color\n",
    "def run_means_assign(palette, new_means, probs):\n",
    "    pal_means = []\n",
    "    size = len(palette)\n",
    "    for i in range(size):\n",
    "        means = means_assign(i, new_means, probs)\n",
    "        pal_means.append(means)\n",
    "    # returns list of list of new means   \n",
    "    return pal_means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "229b6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def means_assign(item_index, new_means, probs):\n",
    "    means_lst = [] # stores items from new_means\n",
    "    for count, x in enumerate(probs):\n",
    "        an_array = np.array(x)\n",
    "        index = np.argmax(an_array) # index of palette color\n",
    "        \n",
    "        if index == item_index: # superpixel was associated with this palette color\n",
    "            means_lst.append(new_means[count])\n",
    "        \n",
    "    # returns sub-set of new_means for each palette color \n",
    "    return means_lst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc3444a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pal_assigns(probs):\n",
    "    assigns = []\n",
    "    for count, x in enumerate(probs):\n",
    "        an_array = np.array(x)\n",
    "        index = np.argmax(an_array)\n",
    "        assigns.append(index)\n",
    "        \n",
    "    # returns clusters palette color assignments \n",
    "    return assigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86c5b7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 600, 3)\n",
      "[[ 2.19338797e+00 -1.39257019e-04  2.63961737e-04]\n",
      " [ 2.19338797e+00 -1.39257019e-04  2.63961737e-04]\n",
      " [ 2.19338797e+00 -1.39257019e-04  2.63961737e-04]\n",
      " ...\n",
      " [ 3.14798880e-01  4.33702381e-01 -1.98422326e+00]\n",
      " [ 3.14798880e-01  4.33702381e-01 -1.98422326e+00]\n",
      " [ 9.89341352e-02  6.95219098e-01 -1.89211773e+00]]\n",
      "fcomponents:\n",
      "[[ 0.69874791 -0.08878177  0.70983742]\n",
      " [ 0.7141013   0.02754238 -0.69950036]\n",
      " [ 0.04255227  0.99567023  0.08264436]]\n",
      "explained variance\n",
      "[438.85178851 322.34919617  34.45607173]\n",
      "explained variance\n",
      "[438.85178851 322.34919617  34.45607173]\n",
      "[[ 22.92148803   1.83303148 -15.96875144]]\n"
     ]
    }
   ],
   "source": [
    "# read the input RGB image\n",
    "rgb = io.imread(\"japan-small.jpg\",plugin='matplotlib')\n",
    "print(rgb.shape)\n",
    "\n",
    "# convert RGB to LAB\n",
    "img = color.rgb2lab(rgb)\n",
    "\n",
    "k = 300   # Number of Super pixels, # adjust later, was 50\n",
    "m = 45    # Constant for normalizing the color proximity, range of m = [1,40], # paper uses '45'\n",
    "\n",
    "img_h = img.shape[0] # Image Height\n",
    "img_w = img.shape[1] # Image Width\n",
    "\n",
    "N = img_h * img_w  # Total number of pixels in the image\n",
    "S = int(math.sqrt(N / k)) # initial length of one superpixel along one dimension\n",
    "\n",
    "flat_img = img.reshape((N,3))\n",
    "\n",
    "# Initialize T and color palette\n",
    "T = 1\n",
    "average = flat_img.mean(axis=0) # avg color of input image\n",
    "# average = img.mean(axis=0).mean(axis=0) # or\n",
    "colors = 8 # user can change\n",
    "palette = makePalette(1, average) # track palette colors\n",
    "pal_size = 1 # initial palette size 1\n",
    "print(palette)\n",
    "image = np.copy(img)\n",
    "# Apply bilateral filter with d = 15, \n",
    "# sigmaColor = sigmaSpace = 75.\n",
    "bilateral = cv2.bilateralFilter(rgb, 15, 75, 75)\n",
    "bilateral = color.rgb2lab(bilateral) # change to LAB space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c02d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = initial_cluster_center(S,img,img_h,img_w) # initialize superpixel clusters as a grid \n",
    "perturb_center(clusters, img) # perturb centers to lowest gradient position\n",
    "assign_color(clusters, palette, 0) # initialize superpixels to first color in palette (avg)\n",
    "print(\"T\")\n",
    "print(T)\n",
    "cycle = 0\n",
    "while T > 0:\n",
    "    print(\"cycle\"+str(cycle))\n",
    "    # Refine superpixels w/ modified slic\n",
    "    assign_pixels(clusters,S,img,img_h,img_w)\n",
    "    update_cluster_mean(clusters, img)\n",
    "    \n",
    "    laplacian(clusters)\n",
    "    new_means = update_mean_color(clusters, bilateral) \n",
    "    \n",
    "    # Palette Iteration\n",
    "    # associate superpixels to palette colors\n",
    "    probs = assign_pal_color(clusters, palette, pal_size)\n",
    "   \n",
    "    # refine color palette\n",
    "    prev_pal = palette.copy() # copy palette to check for convergence in next step\n",
    "    print(\"prev pal:\")\n",
    "    print(palette)\n",
    "    palette = refine_pal(clusters, palette, pal_size, new_means, probs)\n",
    "    print(\"refined pal:\")\n",
    "    print(palette)\n",
    "       \n",
    "    # check palette convergence\n",
    "    answer = if_converg(prev_pal, palette)\n",
    "#     print(answer)\n",
    "    if answer == True:\n",
    "        # reduce temperature by 30 percent\n",
    "        T = T * 0.7 # test\n",
    "        print(T)\n",
    "        # palette size limit reached?\n",
    "        if pal_size == colors:\n",
    "            break\n",
    "        else: # expand palette / resolve splits for each color\n",
    "\n",
    "            # inputs: list of {new means} associated with each existing cluster/Palette Color\n",
    "            new_pal = expand_pal(palette, pal_size, new_means, probs)\n",
    "            pal_size = len(new_pal)\n",
    "            palette = new_pal\n",
    "            print(new_pal)\n",
    "            print(pal_size)\n",
    "            \n",
    "    assigns = pal_assigns(probs)\n",
    "    print(\"p-color assignments:\")\n",
    "    print(assigns)\n",
    "    name = 'japan_m{m}_k{k}_C-{c}.png'.format(m=m, k=k, c=cycle)\n",
    "    avg_color_cluster(img,name,clusters)\n",
    "\n",
    "    cycle += 1\n",
    "           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
